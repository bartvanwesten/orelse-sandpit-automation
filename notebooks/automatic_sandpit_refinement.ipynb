{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68890323",
   "metadata": {},
   "source": [
    "# Automatic Sandpit Refinement for D-Flow FM\n",
    "\n",
    "This notebook provides an automated workflow for refining unstructured grids around sandpit areas in D-Flow FM models. The refinement process uses Casulli refinement to gradually transition from coarse background resolution to fine target resolution.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Configuration**: Set file paths and refinement parameters\n",
    "2. **Load Grid and Polygons**: Load the grid and create/load sandpit polygons\n",
    "3. **Plan Refinement**: Analyze grid resolution and generate refinement zones\n",
    "4. **Execute Refinement**: Apply Casulli refinement to the grid\n",
    "5. **Monitor Quality** (Optional): Analyze grid quality metrics\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- `dfm_tools`\n",
    "- `meshkernel`\n",
    "- `numpy`\n",
    "- `matplotlib`\n",
    "- `shapely`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1b0505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Configuration and Setup\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to Python path (for local installations)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "import numpy as np\n",
    "import dfm_tools as dfmt\n",
    "import matplotlib.pyplot as plt\n",
    "import xugrid as xu\n",
    "import xarray as xr\n",
    "import psutil\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from scipy.spatial.distance import cdist\n",
    "import scipy.interpolate\n",
    "import meshkernel\n",
    "import re\n",
    "\n",
    "# Import utility functions\n",
    "from src.polygon_utils import InteractivePolygonDrawer, load_pol_file, save_pol_file, generate_refinement_polygons, expand_polygon_outward, add_shape_to_polygon\n",
    "from src.refinement_utils import compute_refinement_steps, apply_casulli_refinement, print_refinement_summary\n",
    "from src.visualization_utils import plot_grid, is_codespace\n",
    "from src.monitoring_utils import analyze_grid_quality, plot_grid_quality\n",
    "from src.restart_utils import create_restart_file, compare_restart_files, regrid_restart_data\n",
    "from src.netcdf_utils import export_refined_grid\n",
    "\n",
    "# INPUT: Filenames\n",
    "nc_file ='dcsm_0_125nm_2ref_bathygr7_RGFGRID_net.nc' #'DCSM-FM_0_5nm_grid_20220310_depth_20220517_net.nc'\n",
    "restart_file = 'DCSM-FM_0_5nm_merged_20190127_000000_rst.nc'\n",
    "\n",
    "# INPUT: Toggle options\n",
    "visualization_bool = False      # Set to True to enable visualization\n",
    "monitor_quality = False         # Set to True if you to analyze the quality of the grid\n",
    "use_existing_pol_file = True    # Set to True to use an existing polygon file (no new polygons will be created)\n",
    "plot_bathymetry = False           # Set to True to plot bathymetry in the background of interactive plot\n",
    "\n",
    "# INPUT: Refinement parameters\n",
    "target_resolution = 100  # Target resolution in meters for the finest refinement level\n",
    "buffer_around_sandpit = 250  # Buffer around sandpit polygons in meters\n",
    "N = 6  # Number of transition cells\n",
    "\n",
    "# INPUT: Digging parameters\n",
    "dig_depth = 5.0        # meters to lower bed level\n",
    "slope = 0.03            # slope ratio for smooth transitions (default: 0.1)\n",
    "\n",
    "# Auto-detect correct data path based on current working directory\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "   # Running from notebooks directory (Codespace)\n",
    "   data_path = os.path.join('..', 'data')\n",
    "else:\n",
    "   # Running from project root (local Jupyter)\n",
    "   data_path = 'data'\n",
    "\n",
    "# INPUT: File paths (input & output)\n",
    "nc_path = os.path.join(data_path, 'input', nc_file)\n",
    "restart_path = os.path.join(data_path, 'input', restart_file)\n",
    "input_pol_file = os.path.join(data_path, 'input', 'sandpits.pol')\n",
    "output_pol_file = os.path.join(data_path, 'output', 'sandpits.pol')\n",
    "xyz_output_path = os.path.join(data_path, 'output', 'bathymetry.xyz')\n",
    "output_dir = os.path.join(data_path, 'output')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Environment detection\n",
    "if is_codespace():\n",
    "    print(\"üîß Running in GitHub Codespace\")\n",
    "else:\n",
    "    print(\"üîß Running locally\")\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Target: {target_resolution}m | Buffer: {buffer_around_sandpit}m | Transitions: {N}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132430e9",
   "metadata": {},
   "source": [
    "## Step 1: Load Grid and Define Sandpit Polygons\n",
    "\n",
    "This step loads the D-Flow FM grid and either:\n",
    "- Loads existing sandpit polygons from a .pol file\n",
    "- Opens an interactive drawing tool to create new polygons\n",
    "\n",
    "The polygons define the areas where refinement will be applied.\n",
    "\n",
    "**Note**: Interactive polygon drawing is only available when running locally, not in Codespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b2f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Grid and Create/Load Polygons\n",
    "\n",
    "# Check if NetCDF file exists\n",
    "if not os.path.exists(nc_path):\n",
    "    print(f\"‚ùå NetCDF file not found: {nc_path}\")\n",
    "    if is_codespace():\n",
    "        print(\"   File should be automatically available in Codespace\")\n",
    "    else:\n",
    "        print(\"   Please download the file from: https://github.com/your-repo/releases\")\n",
    "        print(\"   Or use git lfs pull to download LFS files\")\n",
    "    raise FileNotFoundError(f\"Required file not found: {nc_path}\")\n",
    "\n",
    "# Load grid data\n",
    "print(\"üìä Loading grid...\")\n",
    "ugrid = dfmt.open_partitioned_dataset(nc_path)\n",
    "ugrid_original = dfmt.open_partitioned_dataset(nc_path)  # Keep original for later comparison\n",
    "\n",
    "# Convert to meshkernel objects for consistent plotting and refinement\n",
    "mk_object = ugrid.grid.meshkernel\n",
    "mk_backup = ugrid_original.grid.meshkernel  # Keep backup\n",
    "\n",
    "# Handle polygon input/creation\n",
    "if use_existing_pol_file:\n",
    "    # Load existing polygon file\n",
    "    if os.path.exists(input_pol_file):\n",
    "        polygons = load_pol_file(input_pol_file)\n",
    "        print(f\"‚úÖ Loaded {len(polygons)} sandpit polygons from file\")\n",
    "    else:\n",
    "        print(f\"‚ùå File {input_pol_file} not found, switching to interactive mode\")\n",
    "        use_existing_pol_file = False\n",
    "\n",
    "if not use_existing_pol_file:\n",
    "    # Interactive polygon creation\n",
    "    if is_codespace():\n",
    "        print(\"‚ùå Interactive polygon drawing not available in Codespace\")\n",
    "        print(\"   Please create a sandpits.pol file or run locally for interactive drawing\")\n",
    "        raise RuntimeError(\"Interactive mode not supported in Codespace\")\n",
    "    \n",
    "    # Create an interactive plot\n",
    "    %matplotlib tk\n",
    "    print(\"üñ±Ô∏è  Opening interactive polygon drawing tool...\")\n",
    "    print(\"   Instructions: RIGHT CLICK ‚Üí add vertex | ENTER ‚Üí finish polygon | Close window when done\")\n",
    "    \n",
    "    drawer = InteractivePolygonDrawer(ugrid, nc_path, plot_bathymetry)\n",
    "    polygons = drawer.draw_polygons()\n",
    "    \n",
    "    if polygons:\n",
    "        output_file = os.path.join(output_dir, 'sandpits.pol')\n",
    "        save_pol_file(polygons, output_file)\n",
    "        print(f\"‚úÖ Saved {len(polygons)} polygons to {output_file}\")\n",
    "\n",
    "\n",
    "# Uncomment to add one new shape to the existing polygon file\n",
    "add_shape_to_polygon(input_pol_file, output_pol_file, 'rectangle', 4.0, 52.2, width=0.03, height=0.02, rotation=45)\n",
    "polygons = load_pol_file(output_pol_file)  # Reload polygons after adding new shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58feed",
   "metadata": {},
   "source": [
    "## Step 2: Plan Refinement Strategy\n",
    "\n",
    "This step:\n",
    "1. Analyzes the current grid resolution within the sandpit polygons\n",
    "2. Calculates the number of refinement steps needed\n",
    "3. Generates refinement zones with automatic overlap detection and merging\n",
    "4. Visualizes the refinement plan\n",
    "\n",
    "The refinement zones are created from coarse (outer) to fine (inner) resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b552b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Plan Refinement Strategy\n",
    "\n",
    "# Analyze grid resolution and compute refinement steps\n",
    "print(\"üîç Analyzing grid resolution...\")\n",
    "refinement_params = compute_refinement_steps(ugrid, target_resolution, polygons)\n",
    "\n",
    "# Generate refinement polygons with overlap merging\n",
    "print(\"üìê Generating refinement zones...\")\n",
    "(all_refinement_polygons, all_original_polygons, \n",
    " buffer_polygons, expansions) = generate_refinement_polygons(\n",
    "    polygons, refinement_params, buffer_around_sandpit, N)\n",
    "\n",
    "# Store original buffer polygons for visualization\n",
    "original_buffer_polygons = []\n",
    "for i, polygon in enumerate(polygons):\n",
    "    polygon_array = np.array(polygon)\n",
    "    center_lat = np.mean(polygon_array[:, 1])\n",
    "    expanded_polygon = expand_polygon_outward(polygon, buffer_around_sandpit, center_lat)\n",
    "    original_buffer_polygons.append(expanded_polygon)\n",
    "\n",
    "# Visualize refinement plan\n",
    "if visualization_bool:\n",
    "    print(\"üìà Visualizing refinement plan...\")\n",
    "    if not is_codespace():\n",
    "        %matplotlib inline\n",
    "    plot_grid(mk_object, polygons, all_refinement_polygons, all_original_polygons,\n",
    "            buffer_polygons, refinement_params['envelope_sizes_m'], refinement_params['n_steps'],\n",
    "            original_buffer_polygons=original_buffer_polygons,\n",
    "            title='Refinement Plan: Sandpit Polygons and Merged Refinement Zones')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9bd57c",
   "metadata": {},
   "source": [
    "## Step 3: Execute Grid Refinement\n",
    "\n",
    "This step applies Casulli refinement to the meshkernel object using the generated refinement zones. The refinement is applied from outside to inside (coarse to fine) to ensure smooth transitions.\n",
    "\n",
    "After refinement, the refined grid is visualized showing the final mesh structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Execute Grid Refinement\n",
    "\n",
    "# Apply Casulli refinement\n",
    "print(\"‚öôÔ∏è  Executing refinement...\")\n",
    "apply_casulli_refinement(mk_object, all_refinement_polygons)\n",
    "\n",
    "# Visualize refined grid\n",
    "if visualization_bool:\n",
    "    print(\"üìä Visualizing refined grid...\")\n",
    "    if not is_codespace():\n",
    "        %matplotlib inline\n",
    "    plot_grid(mk_object, polygons, all_refinement_polygons, all_original_polygons,\n",
    "            buffer_polygons, refinement_params['envelope_sizes_m'], refinement_params['n_steps'],\n",
    "            title='Refined Grid: Final Result with Casulli Refinement')\n",
    "\n",
    "# Print refinement summary\n",
    "print_refinement_summary(polygons, all_refinement_polygons, \n",
    "                        refinement_params['envelope_sizes_m'], \n",
    "                        refinement_params['n_steps'], buffer_polygons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87140b1c",
   "metadata": {},
   "source": [
    "## Step 4: Monitor Grid Quality (Optional)\n",
    "\n",
    "This optional step analyzes the quality of the refined grid by examining:\n",
    "- **Resolution**: Face areas converted to characteristic lengths\n",
    "- **Smoothness**: Ratio of adjacent cell sizes (target < 1.4)\n",
    "- **Orthogonality**: Deviation from 90¬∞ angles (target < 0.01)\n",
    "\n",
    "**Note**: This analysis can be computationally intensive for large grids, especially in Codespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ef16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: (Optional) Monitor Grid Quality\n",
    "if monitor_quality:\n",
    "    print(\"üî¨ Analyzing grid quality metrics...\")\n",
    "    quality_data = analyze_grid_quality(mk_object, ugrid_original, all_refinement_polygons, polygons)\n",
    "    \n",
    "    print(\"üìä Creating quality visualization...\")\n",
    "    if not is_codespace():\n",
    "        %matplotlib inline\n",
    "        plot_grid_quality(quality_data, all_refinement_polygons, target_resolution)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Apparently the kernel crashes when trying to plot this in a Codespace environment...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_notes",
   "metadata": {},
   "source": [
    "## Step 5: Save Refined Grid (Optional)\n",
    "\n",
    "This step exports the refined grid to a RGFGRID-compatible NetCDF file with automatic versioning to prevent overwriting existing files. The saved file contains complete UGRID topology including all connectivity arrays required by D-Flow FM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "additional_ops",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Export refined grid to NetCDF with proper naming and compatibility\n",
    "\n",
    "output_refined_nc, ugrid_complete = export_refined_grid(\n",
    "    mk_object=mk_object,\n",
    "    original_nc_file=nc_file, \n",
    "    output_dir=output_dir,\n",
    "    suffix=\"_ref\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5a5dfd-9ce4-41df-88d0-09fae8f79320",
   "metadata": {},
   "source": [
    "A. Activities to create new restart file based on new grid:\n",
    "1. Creating new restart file with correct dimensions / coordinate variables\n",
    "2. Horizontal interpolation of all 1D parameters (both elements and links)\n",
    "3. Horizontal interpolation of all 2D parameters (both elements and links)\n",
    "\n",
    "B. Activities to adapt bathymetry after sandpit excavation\n",
    "1. Setup parameterization of sandpit shape (input: depth and slope around edges)\n",
    "2. Lower bed level elevation (FlowElem_bl) & compute excavated volume (before vs after)\n",
    "3. Adapt 1D parameters based on new bed level if necessary (waterdepth, ...?)\n",
    "4. Adapt 2D parameters (primarily vertical profile) on new bed level (flow velocities, turbulence, temperature)\n",
    "\n",
    "C. Fix the partitioning:\n",
    "1. Modify the starting indices and counts (e.g., partitions_face_start) of the partititions based new cell count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd82845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 6a: Export *.xyz file for DFM setup:\n",
    "\n",
    "# # Get coordinates from the refined grid\n",
    "# refined_grid_coords = mk_object.mesh2d_get()\n",
    "\n",
    "# # Get bed level from the restart file\n",
    "# restart_ds = xr.open_dataset(restart_path, decode_timedelta=False)\n",
    "# restart_bed_levels = restart_ds.FlowElem_bl.values[0,:]\n",
    "\n",
    "# # Get coordinates\n",
    "# restart_coords = np.column_stack([restart_ds.FlowElem_xcc.values, restart_ds.FlowElem_ycc.values])\n",
    "# refined_coords = np.column_stack([refined_grid_coords.node_x, refined_grid_coords.node_y])\n",
    "\n",
    "# # Interpolation\n",
    "# bed_levels_interp = scipy.interpolate.griddata(restart_coords, restart_bed_levels, refined_coords, method='linear')\n",
    "# bed_levels_interp_nearest = scipy.interpolate.griddata(restart_coords, restart_bed_levels, refined_coords, method='nearest')\n",
    "# bl_nan = np.isnan(bed_levels_interp)\n",
    "# bed_levels_interp[bl_nan] = bed_levels_interp_nearest[bl_nan]\n",
    "\n",
    "# # 4. Write to *.xyz file\n",
    "# np.savetxt(xyz_output_path, np.column_stack([refined_grid_coords.node_x, refined_grid_coords.node_y, bed_levels_interp]), fmt='%.8f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba34070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 6b: Load xyz file and dig sandpits with slope transitions\n",
    "\n",
    "# transition_distance = dig_depth / slope  # in meters\n",
    "\n",
    "# # Convert buffer distance from meters to degrees (approximate at ~52¬∞N latitude)\n",
    "# lat_approx = 52.0\n",
    "# meters_per_degree_lat = 111320\n",
    "# meters_per_degree_lon = 111320 * np.cos(np.radians(lat_approx))\n",
    "# meters_per_degree_avg = (meters_per_degree_lat + meters_per_degree_lon) / 2\n",
    "# transition_distance_deg = transition_distance / meters_per_degree_avg\n",
    "\n",
    "# # Load xyz file\n",
    "# xyz_data = np.loadtxt(xyz_output_path)\n",
    "# x_coords, y_coords, bed_levels = xyz_data[:, 0], xyz_data[:, 1], xyz_data[:, 2].copy()\n",
    "\n",
    "# # Convert sandpit polygons to shapely and fix invalid polygons\n",
    "# sandpit_polys = []\n",
    "# for i, poly_coords in enumerate(polygons):\n",
    "#     poly = Polygon(poly_coords)\n",
    "#     if not poly.is_valid or poly.area == 0:\n",
    "#         poly = poly.buffer(0)  # Fix self-intersections\n",
    "#         if poly.area == 0:\n",
    "#             from shapely.geometry import MultiPoint\n",
    "#             hull = MultiPoint(poly_coords).convex_hull\n",
    "#             if hasattr(hull, 'area') and hull.area > 0:\n",
    "#                 poly = hull\n",
    "#     sandpit_polys.append(poly)\n",
    "\n",
    "# # Combine polygons\n",
    "# combined_poly = sandpit_polys[0] if len(sandpit_polys) == 1 else gpd.GeoSeries(sandpit_polys).union_all()\n",
    "# grid_points = gpd.GeoDataFrame(geometry=[Point(x, y) for x, y in zip(x_coords, y_coords)])\n",
    "\n",
    "# # Identify excavation zones (transition goes INWARD)\n",
    "# within_sandpit = grid_points.geometry.within(combined_poly)\n",
    "# inner_poly = combined_poly.buffer(-transition_distance_deg)  # Negative buffer = inward\n",
    "# within_inner = grid_points.geometry.within(inner_poly) if hasattr(inner_poly, 'area') and inner_poly.area > 0 else np.zeros(len(grid_points), dtype=bool)\n",
    "# in_transition = within_sandpit & (~within_inner)\n",
    "# in_full_excavation = within_inner\n",
    "\n",
    "# # Apply smooth slope transition\n",
    "# bed_levels_modified = bed_levels.copy()\n",
    "# if in_transition.any():\n",
    "#     # Get and densify boundary coordinates for better distance calculation\n",
    "#     boundary = combined_poly.boundary\n",
    "#     if boundary.geom_type == 'LineString':\n",
    "#         boundary_length = boundary.length\n",
    "#         num_points = max(100, int(boundary_length / transition_distance_deg * 10))\n",
    "#         distances = np.linspace(0, boundary_length, num_points)\n",
    "#         boundary_coords = np.array([boundary.interpolate(d).coords[0] for d in distances])\n",
    "#     else:\n",
    "#         boundary_coords_list = []\n",
    "#         for geom in boundary.geoms:\n",
    "#             geom_length = geom.length\n",
    "#             num_points = max(50, int(geom_length / transition_distance_deg * 10))\n",
    "#             distances = np.linspace(0, geom_length, num_points)\n",
    "#             geom_coords = np.array([geom.interpolate(d).coords[0] for d in distances])\n",
    "#             boundary_coords_list.append(geom_coords)\n",
    "#         boundary_coords = np.vstack(boundary_coords_list)\n",
    "    \n",
    "#     # Calculate distances and apply gradual depth reduction\n",
    "#     transition_points = np.column_stack([x_coords[in_transition], y_coords[in_transition]])\n",
    "#     distances_deg = cdist(transition_points, boundary_coords)\n",
    "#     min_distances_deg = np.min(distances_deg, axis=1)\n",
    "#     min_distances_m = min_distances_deg * meters_per_degree_avg\n",
    "    \n",
    "#     # Points at boundary get minimal excavation, points further inward get more\n",
    "#     depth_reduction = dig_depth * (min_distances_m / transition_distance)\n",
    "#     depth_reduction = np.clip(depth_reduction, 0, dig_depth)\n",
    "#     bed_levels_modified[in_transition] -= depth_reduction\n",
    "\n",
    "# # Apply full depth in inner excavation areas\n",
    "# bed_levels_modified[in_full_excavation] -= dig_depth\n",
    "\n",
    "# # Save modified xyz file\n",
    "# output_path = xyz_output_path.replace('.xyz', '_sandpits_dug.xyz')\n",
    "# np.savetxt(output_path, np.column_stack([x_coords, y_coords, bed_levels_modified]), fmt='%.8f')\n",
    "\n",
    "# # Calculate estimated volume change\n",
    "# depth_changes = bed_levels - bed_levels_modified\n",
    "# excavated_points = depth_changes > 0\n",
    "# cell_area_m2 = target_resolution ** 2\n",
    "# total_excavated_volume_m3 = np.sum(depth_changes[excavated_points]) * cell_area_m2\n",
    "# total_excavated_volume_Mm3 = total_excavated_volume_m3 / 1_000_000  # Convert to million m¬≥\n",
    "\n",
    "# print(f\"Sandpits dug: {in_full_excavation.sum()} points @ {dig_depth}m, {in_transition.sum()} transition points\")\n",
    "# print(f\"Estimated excavated volume: {total_excavated_volume_Mm3:.2f} Mm¬≥ (grid interpolation needed for actual volume)\")\n",
    "# print(f\"Saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f129d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# =============================================================================\n",
    "# 7a: Set DFM model and installation paths\n",
    "# =============================================================================\n",
    "\n",
    "os.chdir(project_root + \"/notebooks\")  # Ensure working directory is notebooks\n",
    "\n",
    "# Model paths (relative)\n",
    "model_dir = Path(\"../data/model/B04_2018_coldstart\")\n",
    "original_mdu = model_dir / \"DCSM-FM_0_5nm.mdu\"\n",
    "dimr_config = model_dir / \"dimr_config.xml\"\n",
    "output_refined_nc = Path(output_refined_nc)\n",
    "\n",
    "# Installation paths (absolute)\n",
    "dflowfm_exe = Path(\"p:/11211460-msc-gw-coupling/02_engines/2025.01/x64/bin/run_dflowfm.bat\")\n",
    "dimr_parallel_exe = Path(\"p:/11211460-msc-gw-coupling/02_engines/2025.01/x64/bin/run_dimr_parallel.bat\")\n",
    "\n",
    "# Partitioning settings\n",
    "n_partitions = 10\n",
    "\n",
    "print(f\"Model directory: {model_dir.resolve()}\")\n",
    "print(f\"Original MDU: {original_mdu}\")\n",
    "print(f\"Refined grid: {output_refined_nc}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7b: Create temporary *.mdu-file with short run-time, high restart-output and correct netcdf\n",
    "# =============================================================================\n",
    "\n",
    "# Create temporary MDU file\n",
    "temp_mdu = model_dir / \"DCSM-FM_0_5nm_temp.mdu\"\n",
    "shutil.copy(original_mdu, temp_mdu)\n",
    "\n",
    "print(f\"Creating temporary MDU file: {temp_mdu}\")\n",
    "\n",
    "# Read and modify MDU file\n",
    "with open(temp_mdu, 'r') as f:\n",
    "    mdu_content = f.read()\n",
    "\n",
    "# Modify key parameters\n",
    "modifications = {\n",
    "    'NetFile': output_refined_nc.name,  # Use refined grid\n",
    "    'Stopdatetime': '20190127001000',  # Short 10 minute run\n",
    "    'MapInterval': '600.',  # Map output every 10 minutes\n",
    "    'RstInterval': '600. 0. 600.',  # Restart output every hour\n",
    "    'HisInterval': '3600.',  # History output every hour\n",
    "    'DtUser': '300.',  # 5-minute forcing update\n",
    "    'DtMax': '60.',  # Max 60s timestep\n",
    "}\n",
    "\n",
    "# Apply modifications to MDU content\n",
    "mdu_lines = mdu_content.split('\\n')\n",
    "for i, line in enumerate(mdu_lines):\n",
    "    for key, value in modifications.items():\n",
    "        if line.strip().startswith(key) and '=' in line:\n",
    "            mdu_lines[i] = f\"{key:<40} = {value:<60} # Modified for partitioning test\"\n",
    "            print(f\"Modified: {key} = {value}\")\n",
    "\n",
    "# Write modified MDU\n",
    "with open(temp_mdu, 'w') as f:\n",
    "    f.write('\\n'.join(mdu_lines))\n",
    "\n",
    "# Copy refined grid to model directory\n",
    "refined_grid_local = model_dir / output_refined_nc.name\n",
    "if not refined_grid_local.exists():\n",
    "    print(\"Copying refined grid to model directory...\")\n",
    "    shutil.copy(output_refined_nc, refined_grid_local)\n",
    "\n",
    "# =============================================================================\n",
    "# 7c: Adjust DIMR config based on number of partitions\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"Modifying DIMR config for {n_partitions} partitions...\")\n",
    "\n",
    "# Read original XML as text and modify it directly to avoid namespace issues\n",
    "with open(dimr_config, 'r', encoding='iso-8859-1') as f:\n",
    "    xml_content = f.read()\n",
    "\n",
    "# Find and replace the process element content\n",
    "process_pattern = r'(<process>)[^<]*(</process>)'\n",
    "process_list = ' '.join(str(i) for i in range(n_partitions))\n",
    "new_process = f'\\\\g<1>{process_list}\\\\g<2>'\n",
    "xml_content = re.sub(process_pattern, new_process, xml_content)\n",
    "\n",
    "# Also update the inputFile to use the temporary MDU\n",
    "xml_content = xml_content.replace('DCSM-FM_0_5nm.mdu', temp_mdu.name)\n",
    "\n",
    "print(f\"Updated process element: {process_list}\")\n",
    "print(f\"Updated inputFile: {temp_mdu.name}\")\n",
    "\n",
    "# Save modified XML\n",
    "temp_dimr_config = model_dir / \"dimr_config_temp.xml\"\n",
    "with open(temp_dimr_config, 'w', encoding='iso-8859-1') as f:\n",
    "    f.write(xml_content)\n",
    "\n",
    "# =============================================================================\n",
    "# 7d: Start partitioning the refined grid (and *.mdu) through batch\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nStarting partitioning into {n_partitions} domains...\")\n",
    "\n",
    "# Change to model directory for partitioning\n",
    "os.chdir(model_dir)\n",
    "\n",
    "# Build partitioning command\n",
    "partition_cmd = [\n",
    "    str(dflowfm_exe),\n",
    "    f'--partition:ndomains={n_partitions}:icgsolver=6',\n",
    "    str(temp_mdu.name)\n",
    "]\n",
    "\n",
    "print(f\"Partition command: {' '.join(partition_cmd)}\")\n",
    "\n",
    "# Run partitioning\n",
    "try:\n",
    "    result = subprocess.run(partition_cmd, capture_output=True, text=True, shell=True)\n",
    "    print(\"Partitioning output:\")\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Partitioning errors:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úì Partitioning completed successfully\")\n",
    "        \n",
    "        # Check if partition files were created\n",
    "        partition_files = list(Path('.').glob(f\"{temp_mdu.stem}_0*{temp_mdu.suffix}\"))\n",
    "        print(f\"Created {len(partition_files)} partition MDU files:\")\n",
    "        for pf in sorted(partition_files):\n",
    "            print(f\"  - {pf}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"‚úó Partitioning failed with return code {result.returncode}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error during partitioning: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 7e: Generate restart files by running the model\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nStarting parallel model run to generate restart files...\")\n",
    "\n",
    "# Build parallel run command\n",
    "parallel_cmd = [\n",
    "    str(dimr_parallel_exe),\n",
    "    str(n_partitions),\n",
    "    str(temp_dimr_config.name)\n",
    "]\n",
    "\n",
    "print(f\"Parallel run command: {' '.join(parallel_cmd)}\")\n",
    "\n",
    "# Run parallel model\n",
    "try:\n",
    "    result = subprocess.run(parallel_cmd, capture_output=True, text=True, shell=True)\n",
    "    print(\"Model run output:\")\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"Model run errors:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úì Model run completed successfully\")\n",
    "        \n",
    "        # Check for restart files\n",
    "        rst_files = list(Path('.').glob(\"*_rst.nc\"))\n",
    "        print(f\"Generated {len(rst_files)} restart files:\")\n",
    "        for rf in sorted(rst_files):\n",
    "            print(f\"  - {rf} ({rf.stat().st_size / 1e6:.1f} MB)\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"‚úó Model run failed with return code {result.returncode}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error during model run: {e}\")\n",
    "\n",
    "print(\"\\nPartitioning and initialization workflow completed!\")\n",
    "\n",
    "os.chdir(project_root + \"/notebooks\")  # Ensure working directory is notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d06dfb-2b0d-4344-9405-6036aab187ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 7: Create New Restart File with Refined Grid\n",
    "\n",
    "# from scipy.spatial import cKDTree\n",
    "\n",
    "# # Load and analyze original restart file\n",
    "# original_restart_ds = xr.open_dataset(restart_path, decode_timedelta=False)\n",
    "\n",
    "# # Check for duplicate coordinates in original grid\n",
    "# grid_coords = np.column_stack([ugrid_original.mesh2d_face_x.values, ugrid_original.mesh2d_face_y.values])\n",
    "# unique_coords, unique_indices = np.unique(grid_coords, axis=0, return_index=True)\n",
    "# duplicates = len(grid_coords) - len(unique_coords)\n",
    "# print(f\"Grid duplicates: {duplicates:,} duplicate coordinates found\")\n",
    "\n",
    "# # Load and combine all dry points\n",
    "# dry_files = [r\"c:\\Users\\weste_bt\\GitHub\\orelse-sandpit-automation\\data\\input\\dry_points\\RW_DryPoints.xyz\",\n",
    "#             r\"c:\\Users\\weste_bt\\GitHub\\orelse-sandpit-automation\\data\\input\\dry_points\\Outer_DryPoints.xyz\", \n",
    "#             r\"c:\\Users\\weste_bt\\GitHub\\orelse-sandpit-automation\\data\\input\\dry_points\\mv2.xyz\",\n",
    "#             r\"c:\\Users\\weste_bt\\GitHub\\orelse-sandpit-automation\\data\\input\\dry_points\\Holland_DryPoints.xyz\"]\n",
    "\n",
    "# dry_points = np.vstack([np.loadtxt(f)[:, :2] for f in dry_files])\n",
    "# print(f\"Loaded {len(dry_points):,} dry points\")\n",
    "\n",
    "# # Filter grid coordinates (keep elements >5m from dry points)\n",
    "# # 5m ‚âà 0.00005 degrees at ~52¬∞N latitude (Netherlands)\n",
    "# threshold_degrees = 0.00005\n",
    "# dry_tree = cKDTree(dry_points)\n",
    "# distances, _ = dry_tree.query(grid_coords)\n",
    "# wet_mask = distances > threshold_degrees\n",
    "# wet_grid_coords = grid_coords[wet_mask]\n",
    "# dry_filtered = (~wet_mask).sum()\n",
    "\n",
    "# print(f\"Dry filtering: {dry_filtered:,} grid points within {threshold_degrees:.6f}¬∞ of dry points\")\n",
    "\n",
    "# # Find unused wet grid elements\n",
    "# restart_coords = np.column_stack([original_restart_ds.FlowElem_xcc.values, original_restart_ds.FlowElem_ycc.values])\n",
    "# wet_tree = cKDTree(wet_grid_coords)\n",
    "# _, used_indices = wet_tree.query(restart_coords)\n",
    "# unused_mask = np.ones(len(wet_grid_coords), dtype=bool)\n",
    "# unused_mask[used_indices] = False\n",
    "\n",
    "# # Save unused elements\n",
    "# unused_coords = wet_grid_coords[unused_mask]\n",
    "# if len(unused_coords) > 0:\n",
    "#    np.savetxt('restart_coords.xyz', np.column_stack([restart_coords, 0*np.ones(len(restart_coords))]), fmt='%.6f')\n",
    "#    np.savetxt('restart_unused.xyz', np.column_stack([unused_coords, 2*np.ones(len(unused_coords))]), fmt='%.6f')\n",
    "#    np.savetxt('ugrid_coords.xyz', np.column_stack([grid_coords, 1*np.ones(len(grid_coords))]), fmt='%.6f')\n",
    "#    np.savetxt('ugrid_wet_coords.xyz', np.column_stack([wet_grid_coords, 3*np.ones(len(wet_grid_coords))]), fmt='%.6f')\n",
    "\n",
    "# print(f\"Grid (all): {len(grid_coords):,} \\nGrid (wet-only): {len(wet_grid_coords):,}\\nRestart (all): {len(restart_coords):,}\\nGrid (unused): {len(unused_coords):,}\")\n",
    "\n",
    "\n",
    "# # Plotting grid mesh and face-coordinates (of both grid and restart files)\n",
    "# fig, ax = plt.subplots(figsize=(16, 12))\n",
    "# mk_backup.mesh2d_get().plot_edges(ax=ax, linewidth=0.5, color='black', alpha=0.3)\n",
    "\n",
    "# ax.scatter(grid_coords[:, 0], grid_coords[:, 1], marker='o', s=80, color='k', label='Input grid - all (*_net.nc)', alpha=0.3, linewidths=0., edgecolors='w')\n",
    "# ax.scatter(restart_coords[:, 0], restart_coords[:, 1], marker='o', s=15, color='tab:red', label='Restart - all (*_rst.nc)', alpha=1)\n",
    "\n",
    "# ax.set_xlim(3.55, 3.8)\n",
    "# ax.set_ylim(53.85, 54.00)\n",
    "# ax.set_aspect('equal')\n",
    "\n",
    "# ax.legend(loc='upper left', fontsize=12)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # # Create analysis structure for comparison\n",
    "# # variable_info = {}\n",
    "# # for var_name, var in original_restart_ds.data_vars.items():\n",
    "# #     var_size_mb = var.size * var.dtype.itemsize / (1024**2)\n",
    "# #     variable_info[var_name] = {\n",
    "# #         'shape': var.shape,\n",
    "# #         'dims': var.dims,\n",
    "# #         'dtype': str(var.dtype),\n",
    "# #         'size_mb': var_size_mb,\n",
    "# #         'attributes': dict(var.attrs)\n",
    "# #     }\n",
    "\n",
    "# # restart_analysis = {\n",
    "# #     'dataset': original_restart_ds,\n",
    "# #     'file_size_gb': file_size_gb,\n",
    "# #     'variable_info': variable_info\n",
    "# # }\n",
    "\n",
    "# # # Create new restart file with all variables\n",
    "# # print(\"üîß Creating new restart file with all variables...\")\n",
    "# # new_restart_template = create_restart_file(ugrid_complete, restart_analysis)\n",
    "\n",
    "# # # After creating the restart file template\n",
    "# # regrid_restart_data(original_restart_ds, new_restart_template, ugrid_complete, ugrid_original)\n",
    "\n",
    "# # # Compare old and new restart files\n",
    "# # compare_restart_files(restart_analysis, new_restart_template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orelse_sandpit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
